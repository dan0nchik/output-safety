import torch
import torch.nn.functional as F
from entities.data import ServiceCheckResult, BotMessage
from langdetect import detect, DetectorFactory
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from use_cases.ports.ml_service import IMLServiceRepository

"""
SAFETY CLASSIFIER MODULE

Purpose:
This module implements a safety classification service for Telegram bot messages to detect harmful, toxic, or disturbing content in user responses. It supports both English and Russian languages.

Models:
- English: "bert-base-uncased" ‚Äî a general-purpose BERT model from HuggingFace used for binary classification (non-toxic vs. toxic).
- Russian: "DeepPavlov/rubert-base-cased" ‚Äî a Russian-language BERT model from DeepPavlov. Supports multilabel classification for various toxicity types (e.g., obscene, insult, threat).

Pipeline Overview:
1. Language Detection ‚Äî detects whether the message is in English or Russian using `langdetect`.
2. Tokenization ‚Äî the input text is tokenized using the corresponding tokenizer for each model.
3. Classification:
   - For English: uses softmax on logits to obtain the probability of toxic content.
   - For Russian: uses sigmoid activation to evaluate multiple toxicity labels, then takes the highest probability among toxic labels.
4. Thresholding ‚Äî if the toxicity score is above 0.3, the message is considered harmful.
5. Masking ‚Äî if harmful, suspicious words are masked in the output (with "***") based on individual word-level toxicity estimation.

"""

DetectorFactory.seed = 42

# === –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å: HateBERT ===
en_model_name = "bert-base-uncased"
en_tokenizer = AutoTokenizer.from_pretrained(en_model_name)
en_model = AutoModelForSequenceClassification.from_pretrained(en_model_name)
en_model.eval()

# === –†—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å: ruBERT-Toxicity ===
ru_model_name = "DeepPavlov/rubert-base-cased"
ru_tokenizer = AutoTokenizer.from_pretrained(ru_model_name)
ru_model = AutoModelForSequenceClassification.from_pretrained(ru_model_name)
ru_model.eval()


class SafetyClassifierRepository(IMLServiceRepository):
    def process(self, message: BotMessage) -> ServiceCheckResult:
        txt = message.answer
        try:
            lang = detect(txt)
        except:
            lang = "unknown"
        if lang == "ru":
            score = predict_toxicity_ru(txt)
        elif lang == "en":
            score = predict_hate_speech_en(txt)

        else:
            score = 0.0
        if score >= 0.3:
            return ServiceCheckResult(False, score, mask_toxic_fragments(txt))
        else:
            return ServiceCheckResult(True, score, txt)


def predict_hate_speech_en(text: str) -> float:
    inputs = en_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = en_model(**inputs)
        probs = F.softmax(outputs.logits, dim=1)
    return probs[0, 1].item()


def predict_toxicity_ru(text: str) -> float:
    inputs = ru_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        logits = ru_model(**inputs).logits
        probs = torch.sigmoid(logits).squeeze().tolist()
    return max(probs[1:])  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º non-toxic


def mask_toxic_fragments(text: str, threshold: float = 0.5) -> str:
    try:
        lang = detect(text)
    except:
        lang = "unknown"

    words = text.split()
    masked = []

    for word in words:
        if lang == "ru":
            tokenizer = ru_tokenizer
            model = ru_model
            with torch.no_grad():
                inputs = tokenizer(word, return_tensors="pt", truncation=True, padding=True)
                logits = model(**inputs).logits
                probs = torch.sigmoid(logits).squeeze().tolist()
                toxicity_score = max(probs[1:])
        elif lang == "en":
            tokenizer = en_tokenizer
            model = en_model
            with torch.no_grad():
                inputs = tokenizer(word, return_tensors="pt", truncation=True, padding=True)
                logits = model(**inputs).logits
                probs = F.softmax(logits, dim=1)
                toxicity_score = probs[0, 1].item()
        else:
            masked.append(word)
            continue

        if toxicity_score > threshold:
            masked.append("***")
        else:
            masked.append(word)

    return ' '.join(masked)


# === –ü—Ä–∏–º–µ—Ä CLI-–ø—Ä–æ–≤–µ—Ä–∫–∏ ===
if __name__ == "__main__":
    test_messages = [
        BotMessage(question="–¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ",
                   answer="I can hear voices in the walls at night.There was blood everywhere. I couldn‚Äôt stop shaking.He said he‚Äôd come for me in my sleep.They found a body in the woods, missing its face.I had a dream where everyone I loved was dead.The smell of burnt flesh still lingers in my mind.She laughed while setting the house on fire.They tortured him for hours. No one stopped them."),
        BotMessage(question="–¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ",
                   answer="–í—Å—é —Å—Ç–µ–Ω—É –∑–∞–ª–∏–ª–æ –∫—Ä–æ–≤—å—é, —è –Ω–µ –º–æ–≥ –¥—ã—à–∞—Ç—å –æ—Ç —É–∂–∞—Å–∞.–û–Ω —Å–∫–∞–∑–∞–ª, —á—Ç–æ —É–±—å—ë—Ç –º–µ–Ω—è, –µ—Å–ª–∏ —è —Ä–∞—Å—Å–∫–∞–∂—É.–Ø —Å–ª—ã—à—É —à–∞–≥–∏ –ø–æ –Ω–æ—á–∞–º, –Ω–æ –¥–æ–º–∞ —è –æ–¥–∏–Ω.–¢–µ–ª–æ –Ω–∞—à–ª–∏ –≤ –ø–æ–¥–≤–∞–ª–µ. –ë–µ–∑ –≥–ª–∞–∑.–ú–Ω–µ —Å–Ω—è—Ç—Å—è —Å–Ω—ã, –≥–¥–µ —è —É–º–∏—Ä–∞—é –∫–∞–∂–¥—ã–π —Ä–∞–∑ –ø–æ-–Ω–æ–≤–æ–º—É.–£ –Ω–µ—ë –±—ã–ª –Ω–æ–∂, –∏ –æ–Ω–∞ —É–ª—ã–±–∞–ª–∞—Å—å.–û–Ω –∫—Ä–∏—á–∞–ª, –Ω–æ –Ω–∏–∫—Ç–æ –Ω–µ –ø—Ä–∏—à—ë–ª.–ú–∞–º–∞ –ø–ª–∞–∫–∞–ª–∞, –∫–æ–≥–¥–∞ —É–≤–∏–¥–µ–ª–∞, —á—Ç–æ —è –Ω–∞—Ä–∏—Å–æ–≤–∞–ª."),
        BotMessage(question="–¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ", answer="–¢—ã –º–æ–ª–æ–¥–µ—Ü"),
        BotMessage(question="–¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ", answer="Go away"),
        BotMessage(question="–¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ", answer="–ò–¥–∏ –∫ —á—ë—Ä—Ç—É"),
    ]
    classifier = SafetyClassifierRepository()
    for msg in test_messages:
        serviceCheckResult = classifier.process(msg)
        print(f"üìù {msg.answer}")
        print(f"   ‚û§ –¢–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å: {serviceCheckResult.score:.4f}")
        print(f"   ‚û§ –ó–∞—â–∏—Ç–∞: {serviceCheckResult.masked_answer}\n")
